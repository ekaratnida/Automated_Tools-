{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "nz_convnet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "nteract": {
      "version": "0.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5lGLV8QePSF"
      },
      "source": [
        "# Part 0 - Intro"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "66-15hZ5n4Zc",
        "outputId": "34c5e095-76be-457a-8c76-0864cf08dd91"
      },
      "source": [
        "## On Google Colaboratory only, install google drive stuff\n",
        "'''\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "#'''"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\\n!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\\n!apt-get update -qq 2>&1 > /dev/null\\n!apt-get -y install -qq google-drive-ocamlfuse fuse\\nfrom google.colab import auth\\nauth.authenticate_user()\\nfrom oauth2client.client import GoogleCredentials\\ncreds = GoogleCredentials.get_application_default()\\nimport getpass\\n!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\\nvcode = getpass.getpass()\\n!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\\n#'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9g04cjQMn56O",
        "outputId": "c5131016-8bb1-460a-e908-fe8830e6e5aa"
      },
      "source": [
        "## On Google Colaboratory only, link google drive\n",
        "'''\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "#'''"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n!mkdir -p drive\\n!google-drive-ocamlfuse drive\\n#'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FrjWr__eixk"
      },
      "source": [
        "## On Google Colaboratory only, install necessary dependencies\n",
        "#!pip install keras tensorflow-gpu && apt update && apt install -y python3-gdal"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qI5gAk9lePSI"
      },
      "source": [
        "# Set number of GPUs\n",
        "num_gpus = 4   #defaults to 1 if one-GPU or one-CPU. If 4 GPUs, set to 4.\n",
        "\n",
        "# Set height (y-axis length) and width (x-axis length) to train model on\n",
        "img_height, img_width = (256,256)  #Default to (256,256), use (None,None) if you do not want to resize imgs"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "fRRjiHfCePST",
        "outputId": "58eb5b8d-bbd8-49b5-bfee-bab100efbf44"
      },
      "source": [
        "# Import all the necessary libraries\n",
        "import os\n",
        "import datetime\n",
        "import glob\n",
        "import random\n",
        "import re\n",
        "import sys\n",
        "import time\n",
        "import zipfile\n",
        "\n",
        "import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import skimage.io                                     #Used for imshow function\n",
        "import skimage.transform                              #Used for resize function\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "import dask.array as da\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "import keras\n",
        "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, Conv2DTranspose\n",
        "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D, Lambda, AlphaDropout\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.models import load_model, Model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers.merge import add, concatenate\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "#from keras.utils import multi_gpu_model\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras import backend as K\n",
        "\n",
        "import tensorflow as tf\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import ogr\n",
        "import gdal\n",
        "from osgeo import gdal_array\n",
        "\n",
        "print('Python       :', sys.version.split('\\n')[0])\n",
        "print('Numpy        :', np.__version__)\n",
        "print('Skimage      :', skimage.__version__)\n",
        "print('Scikit-learn :', sklearn.__version__)\n",
        "print('Keras        :', keras.__version__)\n",
        "print('Tensorflow   :', tf.__version__)\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python       : 3.7.12 (default, Sep 10 2021, 00:21:48) \n",
            "Numpy        : 1.19.5\n",
            "Skimage      : 0.18.3\n",
            "Scikit-learn : 1.0.1\n",
            "Keras        : 2.7.0\n",
            "Tensorflow   : 2.7.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_C6ByviePSo"
      },
      "source": [
        "# Set seed values\n",
        "seed = 42\n",
        "random.seed = seed\n",
        "np.random.seed(seed=seed)\n",
        "tf.random.set_seed(seed=seed)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rF2A1OMePSs"
      },
      "source": [
        "# Have a look at our data folder\n",
        "topDir = os.getcwd()+\"/data\"  #default top directory\n",
        "#topDir = \"/content/drive/Colab Notebooks/data\"  #default top directory on Google Colab\n",
        "os.chdir(topDir)\n",
        "print(os.listdir())\n",
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NMEiG8BePSx"
      },
      "source": [
        "# Part 1 - Data Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovf2b4LgePSz"
      },
      "source": [
        "# https://pcjericks.github.io/py-gdalogr-cookbook/raster_layers.html\n",
        "# https://gis.stackexchange.com/questions/16837/how-can-i-turn-a-shapefile-into-a-mask-and-calculate-the-mean\n",
        "def vectorPoly_to_rasterMask(vector, raster, suffix='_mask', output=None, show=False):\n",
        "    \"\"\"\n",
        "    Function to turn a vector Polygon (ogr) into a raster binary mask (gdal) of 1 for present, 0 for absent\n",
        "    \n",
        "    Outputs a raster geotiff with extents according to the input raster.\n",
        "    \"\"\"\n",
        "    if output==None:\n",
        "        output = raster.split('.')[0]+suffix+\".\"+raster.split('.')[-1]\n",
        "    \n",
        "    \n",
        "    ## Open raster data source\n",
        "    assert(os.path.exists(raster))\n",
        "    raster_ds = gdal.Open(raster)\n",
        "    raster_prj = raster_ds.GetProjection()\n",
        "    \n",
        "    ulx, px, rx, uly, ry, py = raster_ds.GetGeoTransform()  #upper left X, pixel resolution X, rotation X, upper left Y, pixel resolution Y, rotation Y\n",
        "    px, py = round(px,2), round(py,2)  #round pixel size to two decimal places\n",
        "    width, height = raster_ds.RasterXSize, raster_ds.RasterYSize  #number of x columns and number of y rows\n",
        "    #print(ulx, px, rx, uly, ry, py), (width, height)\n",
        "    '''\n",
        "\n",
        "      ul-------ur\n",
        "    ^  |       |\n",
        "    |  |  geo  |    y increases going up, x increases going right\n",
        "    y  |       |\n",
        "      ll-------lr\n",
        "          x-->\n",
        "\n",
        "    '''\n",
        "    assert(rx==0 and ry==0)   #assuming zero rotation!!\n",
        "    llx = ulx\n",
        "    lly = uly + (height * py)\n",
        "    urx = ulx + (width * px)\n",
        "    ury = uly\n",
        "    bbox = (llx, lly, urx, ury)  #minx, miny, maxx, maxy\n",
        "    #print(\"min_xy:({0},{1}), max_xy:({2},{3})\".format(*bbox))\n",
        "    \n",
        "    \n",
        "    ## Open vector data source\n",
        "    assert(os.path.exists(vector))\n",
        "    vector_ds = ogr.Open(vector)\n",
        "    vector_lyr = vector_ds.GetLayer()\n",
        "    vector_prj = vector_lyr.GetSpatialRef()\n",
        "    vector_ext = vector_lyr.GetExtent()   #x_min, x_max, y_min, y_max\n",
        "    #print(vector_ext)\n",
        "    \n",
        "    \n",
        "    ## Create the raster mask according to input raster dimensions\n",
        "    #print((urx - llx) / px, (ury - lly) / abs(py))\n",
        "    x_res = int(round((urx - llx) / px, 0))\n",
        "    y_res = int(round((ury - lly) / abs(py), 0))  #turn negative pixel size y to absolute value\n",
        "    #print(x_res, y_res)\n",
        "    target_ds = gdal.GetDriverByName('GTiff').Create(output, x_res, y_res, 1, gdal.GDT_Byte)\n",
        "    target_ds.SetGeoTransform((llx, px, rx, uly, ry, py))\n",
        "    target_ds.SetProjection(raster_prj)\n",
        "    band = target_ds.GetRasterBand(1)\n",
        "    band.SetNoDataValue(np.nan)\n",
        "    \n",
        "    ## Rasterize\n",
        "    err = gdal.RasterizeLayer(target_ds, [1], vector_lyr, None, None, [1], ['ALL_TOUCHED=TRUE'])\n",
        "    target_ds.FlushCache()\n",
        "    \n",
        "    \n",
        "    ## Create output arrays\n",
        "    img_ary = np.dstack([raster_ds.GetRasterBand(i).ReadAsArray() for i in range(1,4)])\n",
        "    msk_ary = target_ds.GetRasterBand(1).ReadAsArray()\n",
        "   \n",
        "    \n",
        "    ## Visualize the raster with its output mask\n",
        "    if show==True:\n",
        "        #f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
        "        skimage.io.imshow(img_ary)\n",
        "        plt.show()\n",
        "        skimage.io.imshow(msk_ary)\n",
        "        plt.show()\n",
        "        #skimage.io.imsave(output, mask_ary)\n",
        "    \n",
        "    ## Final checks and turn mask into boolean array\n",
        "    #print(img_ary.shape[:2], msk_ary.shape)\n",
        "    assert(img_ary.shape[:2]==msk_ary.shape)   #check that shape of image and mask are the same\n",
        "    msk_ary = skimage.transform.resize(msk_ary, output_shape=msk_ary.shape+(1,), mode='constant', preserve_range=True)  #need to add an extra dimension so mask.shape = (img_height, img_width, 1)\n",
        "    msk_ary = msk_ary.astype(bool)  #convert to binary mask of either 0 or 1\n",
        "    \n",
        "    return img_ary, msk_ary\n",
        "\n",
        "def ary_to_tiles(ary, shape=(256,256), exclude_empty=False):\n",
        "    \"\"\"\n",
        "    Function to turn a big 2D numpy array (image) and tile it into a set number of shapes\n",
        "    \n",
        "    Outputs a stacked numpy array suitable for input into a Convolutional Neural Network\n",
        "    \"\"\"\n",
        "    assert(isinstance(ary, np.ndarray))\n",
        "    assert(isinstance(shape, tuple))\n",
        "    \n",
        "    ary_height, ary_width = shape\n",
        "    ary_list = []\n",
        "    \n",
        "    total = 0\n",
        "    excluded = 0\n",
        "    for x_step in range(0, ary.shape[1], ary_width):\n",
        "        for y_step in range(0, ary.shape[0], ary_height):\n",
        "            x0, x1 = x_step, x_step+ary_width\n",
        "            y0, y1 = y_step, y_step+ary_height\n",
        "            \n",
        "            crop_ary = ary[y0:y1, x0:x1]\n",
        "            try:\n",
        "                total += 1\n",
        "                assert(crop_ary.shape == (ary_height, ary_width, ary.shape[2]))  #do not include images not matching the intended size\n",
        "            except AssertionError:\n",
        "                excluded += 1\n",
        "                #print(y0,y1,x0,x1, 'excluded')\n",
        "                continue\n",
        "            ary_list.append(crop_ary)\n",
        "    \n",
        "    if excluded > 0:\n",
        "        print(\"INFO: {0}/{1} tiles were excluded due to not fitting shape {2}\".format(excluded, total, shape))\n",
        "    return np.stack(ary_list), excluded\n",
        "\n",
        "splitName = lambda filename: filename.split(os.sep)[-1].split('.')[0]   #function to strip filename of its directories and extension"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7bYLFN6CRth"
      },
      "source": [
        "uzFullPath = topDir+\"/raster/\"  #full path to location of the will-be unzipped files\n",
        "with tqdm.tqdm(total=len(glob.glob(topDir+\"/raster/downloads/*.zip\"))) as pbar:\n",
        "    for i, zFile in enumerate(sorted(glob.glob(topDir+\"/raster/downloads/*.zip\"))):\n",
        "        pbar.set_description('processing: {0}'.format(splitName(zFile)))\n",
        "        pbar.update(1)\n",
        "        with zipfile.ZipFile(file=zFile) as zf:\n",
        "            for eachFile in zf.infolist():\n",
        "                assert(len(eachFile.filename.split('/')) == 1)\n",
        "                if not os.path.exists(os.path.join(uzFullPath, eachFile.filename)):\n",
        "                    #print(\"Unzipping\", eachFile.filename)\n",
        "                    zf.extract(eachFile.filename, uzFullPath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLpnp3pJCRti"
      },
      "source": [
        "if not os.path.exists('train/X_data.npz') or not os.path.exists('train/Y_data.npz'):\n",
        "    with tqdm.tqdm(total=len(glob.glob(topDir+\"/raster/downloads/*.zip\"))) as pbar:\n",
        "        for i, raster in enumerate(sorted(glob.glob(topDir+\"/raster/*[!_mask].tif\"))):\n",
        "            tifName = splitName(raster)\n",
        "            pbar.set_description('processing: {0}'.format(tifName))\n",
        "            pbar.update(1)\n",
        "            if not os.path.exists(\"train/X_{0}.hdf5\".format(tifName)):\n",
        "                img_ary_tmp, msk_ary_tmp = vectorPoly_to_rasterMask(vector='vector/nz-building-outlines-pilot.shp', raster=raster)\n",
        "                # Generate stacked numpy tile blocks of size (img_height, img_width) from one geotiff\n",
        "                X_data_tmp, _ = ary_to_tiles(img_ary_tmp, shape=(img_height, img_width))\n",
        "                Y_data_tmp, _ = ary_to_tiles(msk_ary_tmp, shape=(img_height, img_width))\n",
        "                # Convert stacked numpy array to dask array\n",
        "                X_da_tmp = da.from_array(X_data_tmp, chunks=(32,img_height,img_width,1))\n",
        "                Y_da_tmp = da.from_array(Y_data_tmp, chunks=(32,img_height,img_width,1))\n",
        "                # Save dask array to HDF5 file format with maximum compression\n",
        "                X_da_tmp.to_hdf5(filename=\"train/X_{0}.hdf5\".format(tifName), datapath=tifName, compression=\"gzip\", compression_opts=9)\n",
        "                Y_da_tmp.to_hdf5(filename=\"train/Y_{0}.hdf5\".format(tifName), datapath=tifName, compression=\"gzip\", compression_opts=9)\n",
        "            \n",
        "    xdsets = [h5py.File(fn)['/'+splitName(fn)[2:]] for fn in sorted(glob.glob(\"train/X_*.hdf5\"))]\n",
        "    ydsets = [h5py.File(fn)['/'+splitName(fn)[2:]] for fn in sorted(glob.glob(\"train/Y_*.hdf5\"))]\n",
        "    xarys = [da.from_array(dset, chunks=(32,img_height,img_width,1)) for dset in xdsets]\n",
        "    yarys = [da.from_array(dset, chunks=(32,img_height,img_width,1)) for dset in ydsets]\n",
        "    xdata = da.concatenate(xarys, axis=0)  # Concatenate arrays along first axis\n",
        "    ydata = da.concatenate(yarys, axis=0)  # Concatenate arrays along first axis\n",
        "    # Clean up empty masks\n",
        "    zero_mask = (ydata == 0).all(axis=(1,2,3))\n",
        "    np.savez('train/Y_data.npz', Y_data=ydata[~zero_mask].compute())  #apply zero_mask to Y_train (masks) and save to uncompressed numpy format\n",
        "    np.savez('train/X_data.npz', X_data=xdata[~zero_mask].compute())  #apply zero_mask to X_train (images) and save to uncompressed numpy format\n",
        "    \n",
        "if os.path.exists('train/X_data.npz') and os.path.exists('train/Y_data.npz'):\n",
        "    X_data = np.load('train/X_data.npz')['X_data']\n",
        "    Y_data = np.load('train/Y_data.npz')['Y_data']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUWqcTm6ePS9"
      },
      "source": [
        "print(X_data.shape, X_data.dtype)\n",
        "print(Y_data.shape, Y_data.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JVZ7SdFePTJ"
      },
      "source": [
        "## Visualize masks on the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxXXzV4UePTO"
      },
      "source": [
        "id = 128\n",
        "print(X_data[id].shape)\n",
        "skimage.io.imshow(X_data[id])\n",
        "plt.show()\n",
        "skimage.io.imshow(Y_data[id][:,:,0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_wbpZL4ePTX"
      },
      "source": [
        "# Part 2 - Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xuKiVKtePTY"
      },
      "source": [
        "# Custom IoU metric\n",
        "def mean_iou(y_true, y_pred):\n",
        "    prec = []\n",
        "    for t in np.arange(0.50, 1.0, 0.05):\n",
        "        y_pred_ = tf.to_int32(y_pred > t)\n",
        "        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
        "        K.get_session().run(tf.local_variables_initializer())\n",
        "        with tf.control_dependencies([up_opt]):\n",
        "            score = tf.identity(score)\n",
        "        prec.append(score) \n",
        "        return K.mean(K.stack(prec), axis=0)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbJhtGqDePTd"
      },
      "source": [
        "# Design our model architecture here\n",
        "def keras_model(img_width=256, img_height=256, tensorboard_images=False):\n",
        "    '''\n",
        "    Modified from https://keunwoochoi.wordpress.com/2017/10/11/u-net-on-keras-2-0/\n",
        "    Architecture inspired by https://blog.deepsense.ai/deep-learning-for-satellite-imagery-via-image-segmentation/\n",
        "    '''\n",
        "    #n_ch_exps = [4, 5, 6, 7, 8]   #the n-th deep channel's exponent i.e. 2**n 16,32,64,128,256\n",
        "    n_ch_exps = [6, 6, 6, 6, 6]\n",
        "    k_size = (3, 3)                     #size of filter kernel\n",
        "    k_init = 'lecun_normal'             #kernel initializer\n",
        "    activation = 'selu'\n",
        "\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        ch_axis = 1\n",
        "        input_shape = (3, img_width, img_height)\n",
        "    elif K.image_data_format() == 'channels_last':\n",
        "        ch_axis = 3\n",
        "        input_shape = (img_width, img_height, 3)\n",
        "\n",
        "    inp = Input(shape=input_shape)\n",
        "    if tensorboard_images == True:\n",
        "        tf.summary.image(name='input', tensor=inp)\n",
        "    encodeds = []\n",
        "\n",
        "    # encoder\n",
        "    enc = inp\n",
        "    print(n_ch_exps)\n",
        "    for l_idx, n_ch in enumerate(n_ch_exps):\n",
        "        with K.name_scope('Encode_block_'+str(l_idx)):\n",
        "            enc = Conv2D(filters=2**n_ch, kernel_size=k_size, activation=activation, padding='same', kernel_initializer=k_init)(enc)\n",
        "            enc = AlphaDropout(0.1*l_idx,)(enc)\n",
        "            enc = Conv2D(filters=2**n_ch, kernel_size=k_size, dilation_rate=(2,2), activation=activation, padding='same', kernel_initializer=k_init)(enc)\n",
        "            encodeds.append(enc)\n",
        "            #print(l_idx, enc)\n",
        "            if l_idx < len(n_ch_exps)-1:  #do not run max pooling on the last encoding/downsampling step\n",
        "                enc = MaxPooling2D(pool_size=(2,2))(enc)  #strides = pool_size if strides is not set\n",
        "                #enc = Conv2D(filters=2**n_ch, kernel_size=k_size, strides=(2,2), activation=activation, padding='same', kernel_initializer=k_init)(enc)\n",
        "            if tensorboard_images == True:\n",
        "                tf.summary.histogram(\"conv_encoder\", enc)\n",
        "            \n",
        "    # decoder\n",
        "    dec = enc\n",
        "    print(n_ch_exps[::-1][1:])\n",
        "    decoder_n_chs = n_ch_exps[::-1][1:]\n",
        "    for l_idx, n_ch in enumerate(decoder_n_chs):\n",
        "        with K.name_scope('Decode_block_'+str(l_idx)):\n",
        "            l_idx_rev = len(n_ch_exps) - l_idx - 1  #\n",
        "            dec = concatenate([dec, encodeds[l_idx_rev]], axis=ch_axis)\n",
        "            dec = Conv2D(filters=2**n_ch, kernel_size=k_size, dilation_rate=(2,2), activation=activation, padding='same', kernel_initializer=k_init)(dec)\n",
        "            dec = AlphaDropout(0.1*l_idx)(dec)\n",
        "            dec = Conv2D(filters=2**n_ch, kernel_size=k_size, activation=activation, padding='same', kernel_initializer=k_init)(dec)\n",
        "            dec = Conv2DTranspose(filters=2**n_ch, kernel_size=k_size, strides=(2,2), activation=activation, padding='same', kernel_initializer=k_init)(dec)\n",
        "\n",
        "    outp = Conv2DTranspose(filters=1, kernel_size=k_size, activation='sigmoid', padding='same', kernel_initializer='glorot_normal')(dec)\n",
        "    if tensorboard_images == True:\n",
        "        tf.summary.image(name='output', tensor=outp)\n",
        "    \n",
        "    model = Model(inputs=[inp], outputs=[outp])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LqjzxygePTj",
        "scrolled": false,
        "outputId": "c765f914-b998-487b-a270-76e7ea3dfa22"
      },
      "source": [
        "# Set some model compile parameters\n",
        "#optimizer = keras.optimizers.Adam()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss      = 'binary_crossentropy'\n",
        "metrics   = [mean_iou]\n",
        "\n",
        "# Compile our model\n",
        "model = keras_model(img_width=img_width, img_height=img_height)\n",
        "model.summary()\n",
        "\n",
        "# For more GPUs\n",
        "#if num_gpus > 1:\n",
        "#    model = multi_gpu_model(model, gpus=num_gpus)\n",
        "    \n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6, 6, 6, 6, 6]\n",
            "[6, 6, 6, 6]\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 256, 256, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 256, 256, 64  1792        ['input_3[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " alpha_dropout_9 (AlphaDropout)  (None, 256, 256, 64  0          ['conv2d_18[0][0]']              \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 256, 256, 64  36928       ['alpha_dropout_9[0][0]']        \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d_4 (MaxPooling2D)  (None, 128, 128, 64  0          ['conv2d_19[0][0]']              \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 128, 128, 64  36928       ['max_pooling2d_4[0][0]']        \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " alpha_dropout_10 (AlphaDropout  (None, 128, 128, 64  0          ['conv2d_20[0][0]']              \n",
            " )                              )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 128, 128, 64  36928       ['alpha_dropout_10[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d_5 (MaxPooling2D)  (None, 64, 64, 64)  0           ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 64, 64, 64)   36928       ['max_pooling2d_5[0][0]']        \n",
            "                                                                                                  \n",
            " alpha_dropout_11 (AlphaDropout  (None, 64, 64, 64)  0           ['conv2d_22[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 64, 64, 64)   36928       ['alpha_dropout_11[0][0]']       \n",
            "                                                                                                  \n",
            " max_pooling2d_6 (MaxPooling2D)  (None, 32, 32, 64)  0           ['conv2d_23[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 32, 32, 64)   36928       ['max_pooling2d_6[0][0]']        \n",
            "                                                                                                  \n",
            " alpha_dropout_12 (AlphaDropout  (None, 32, 32, 64)  0           ['conv2d_24[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 32, 32, 64)   36928       ['alpha_dropout_12[0][0]']       \n",
            "                                                                                                  \n",
            " max_pooling2d_7 (MaxPooling2D)  (None, 16, 16, 64)  0           ['conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 16, 16, 64)   36928       ['max_pooling2d_7[0][0]']        \n",
            "                                                                                                  \n",
            " alpha_dropout_13 (AlphaDropout  (None, 16, 16, 64)  0           ['conv2d_26[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 16, 16, 64)   36928       ['alpha_dropout_13[0][0]']       \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate)    (None, 16, 16, 128)  0           ['conv2d_27[0][0]',              \n",
            "                                                                  'conv2d_27[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 16, 16, 64)   73792       ['concatenate_4[0][0]']          \n",
            "                                                                                                  \n",
            " alpha_dropout_14 (AlphaDropout  (None, 16, 16, 64)  0           ['conv2d_28[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 16, 16, 64)   36928       ['alpha_dropout_14[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_transpose_5 (Conv2DTran  (None, 32, 32, 64)  36928       ['conv2d_29[0][0]']              \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate)    (None, 32, 32, 128)  0           ['conv2d_transpose_5[0][0]',     \n",
            "                                                                  'conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 32, 32, 64)   73792       ['concatenate_5[0][0]']          \n",
            "                                                                                                  \n",
            " alpha_dropout_15 (AlphaDropout  (None, 32, 32, 64)  0           ['conv2d_30[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 32, 32, 64)   36928       ['alpha_dropout_15[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_transpose_6 (Conv2DTran  (None, 64, 64, 64)  36928       ['conv2d_31[0][0]']              \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " concatenate_6 (Concatenate)    (None, 64, 64, 128)  0           ['conv2d_transpose_6[0][0]',     \n",
            "                                                                  'conv2d_23[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 64, 64, 64)   73792       ['concatenate_6[0][0]']          \n",
            "                                                                                                  \n",
            " alpha_dropout_16 (AlphaDropout  (None, 64, 64, 64)  0           ['conv2d_32[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 64, 64, 64)   36928       ['alpha_dropout_16[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_transpose_7 (Conv2DTran  (None, 128, 128, 64  36928      ['conv2d_33[0][0]']              \n",
            " spose)                         )                                                                 \n",
            "                                                                                                  \n",
            " concatenate_7 (Concatenate)    (None, 128, 128, 12  0           ['conv2d_transpose_7[0][0]',     \n",
            "                                8)                                'conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 128, 128, 64  73792       ['concatenate_7[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " alpha_dropout_17 (AlphaDropout  (None, 128, 128, 64  0          ['conv2d_34[0][0]']              \n",
            " )                              )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 128, 128, 64  36928       ['alpha_dropout_17[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_transpose_8 (Conv2DTran  (None, 256, 256, 64  36928      ['conv2d_35[0][0]']              \n",
            " spose)                         )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_transpose_9 (Conv2DTran  (None, 256, 256, 1)  577        ['conv2d_transpose_8[0][0]']     \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 925,313\n",
            "Trainable params: 925,313\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCkN9qnRePTt"
      },
      "source": [
        "# Part 3 - Run model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1zn2EfsePTv"
      },
      "source": [
        "# Runtime custom callbacks\n",
        "\n",
        "# Live loss plot from https://github.com/deepsense-ai/intel-ai-webinar-neural-networks/blob/master/live_loss_plot.py\n",
        "# Fixed code to enable non-flat loss plots on keras model.fit_generator()\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import Callback\n",
        "from IPython.display import clear_output\n",
        "#from matplotlib.ticker import FormatStrFormatter\n",
        "\n",
        "def translate_metric(x):\n",
        "    translations = {'acc': \"Accuracy\", 'loss': \"Log-loss (cost function)\"}\n",
        "    if x in translations:\n",
        "        return translations[x]\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "class PlotLosses(Callback):\n",
        "    def __init__(self, figsize=None):\n",
        "        super(PlotLosses, self).__init__()\n",
        "        self.figsize = figsize\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "\n",
        "        self.base_metrics = [metric for metric in self.params['metrics'] if not metric.startswith('val_')]\n",
        "        self.logs = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.logs.append(logs.copy())\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        plt.figure(figsize=self.figsize)\n",
        "        \n",
        "        for metric_id, metric in enumerate(self.base_metrics):\n",
        "            plt.subplot(1, len(self.base_metrics), metric_id + 1)\n",
        "            \n",
        "            plt.plot(range(1, len(self.logs) + 1),\n",
        "                     [log[metric] for log in self.logs],\n",
        "                     label=\"training\")\n",
        "            if self.params['do_validation']:\n",
        "                plt.plot(range(1, len(self.logs) + 1),\n",
        "                         [log['val_' + metric] for log in self.logs],\n",
        "                         label=\"validation\")\n",
        "            plt.title(translate_metric(metric))\n",
        "            plt.xlabel('epoch')\n",
        "            plt.legend(loc='center left')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show();\n",
        "\n",
        "plot_losses = PlotLosses(figsize=(16, 4))\n",
        "\n",
        "# Tensorboard\n",
        "tensorboard = keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=1, write_graph=True, write_images=True)\n",
        "\n",
        "# Model Checkpoints\n",
        "if not os.path.exists(topDir+\"/model\"):\n",
        "    os.makedirs(topDir+\"/model\")\n",
        "filepath=\"model/weights-{epoch:02d}-{val_mean_iou:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_mean_iou', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
        "\n",
        "# Bring all the callbacks together into a python list\n",
        "callbackList = [plot_losses, tensorboard, checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0unalDgLePT0"
      },
      "source": [
        "validation_split = 0.20\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_data,\n",
        "                                                    Y_data,\n",
        "                                                    train_size=1-validation_split,\n",
        "                                                    test_size=validation_split,\n",
        "                                                    random_state=seed)\n",
        "print(\"X_train: {0} tiles, X_test: {1} tiles\".format(len(X_train), len(X_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMU4RkHzePT3",
        "scrolled": false
      },
      "source": [
        "# Finally train the model!!\n",
        "batch_size = 32\n",
        "epochs = 300\n",
        "\n",
        "try:  #to load a checkpoint file if it exists\n",
        "    checkfile = sorted(glob.glob(topDir+\"/model/weights-*-*.hdf5\"))[-1]\n",
        "    model.load_weights(checkfile)\n",
        "    initial_epoch = int(re.search(r\"weights-(\\d*)-\", checkfile).group(1))\n",
        "    print(\"Model weights loaded, resuming from epoch {0}\".format(initial_epoch))\n",
        "except IndexError:\n",
        "    initial_epoch = 0\n",
        "    try:\n",
        "        model.load_weights(topDir+\"/model/model-weights.hdf5\")\n",
        "        print(\"Model weights loaded, starting from epoch {0}\".format(initial_epoch))\n",
        "    except OSError:\n",
        "        pass\n",
        "\n",
        "model.fit(x=X_train, y=Y_train, verbose=1, validation_split=0.25, batch_size=batch_size, epochs=epochs, callbacks=callbackList, initial_epoch=initial_epoch)\n",
        "#model.fit(x=X_train, y=Y_train, verbose=1, validation_data=(X_test, Y_test), batch_size=batch_size, epochs=epochs, callbacks=callbackList)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-I8-GyIeePT-"
      },
      "source": [
        "# Save the model weights to a hdf5 file\n",
        "if num_gpus > 1:\n",
        "    #Refer to https://stackoverflow.com/questions/41342098/keras-load-checkpoint-weights-hdf5-generated-by-multiple-gpus\n",
        "    #model.summary()\n",
        "    model_out = model.layers[-2]  #get second last layer in multi_gpu_model i.e. model.get_layer('model_1')\n",
        "else:\n",
        "    model_out = model\n",
        "model_out.save_weights(filepath=topDir+\"/model/model-weights.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckocaDHHePUE"
      },
      "source": [
        "# Part 4 - Evaluate output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "JwoopOWDePUH",
        "outputId": "84ddba8a-3309-4a45-82f8-8ae0539c48cf"
      },
      "source": [
        "# Reload the model\n",
        "model_loaded = keras_model(img_width=img_width, img_height=img_height)\n",
        "model_loaded.load_weights(filepath=topDir+\"/model/model-weights.hdf5\")\n",
        "model_loaded.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6, 6, 6, 6, 6]\n",
            "[6, 6, 6, 6]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-20833998e8d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Reload the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_loaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_height\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel_loaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopDir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/model/model-weights.hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel_loaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'topDir' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF9LTxzpCRtv"
      },
      "source": [
        "## Visualize predictions on the cross-validation test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R32UCqaDePUe"
      },
      "source": [
        "# Use model to predict test labels\n",
        "Y_hat_test = model_loaded.predict(X_test, verbose=1)\n",
        "print(Y_hat_test.shape, Y_hat_test.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrI-GJVWePUl",
        "scrolled": false
      },
      "source": [
        "for i in range(10):\n",
        "    try:\n",
        "        id = random.randrange(0,len(Y_hat_test))\n",
        "        print(id, X_test[id].shape)\n",
        "        fig, axarr = plt.subplots(nrows=1, ncols=3, squeeze=False, figsize=(20,20))\n",
        "        axarr[0, 0].imshow(X_test[id])\n",
        "        axarr[0, 1].imshow(Y_hat_test[id][:,:,0])\n",
        "        axarr[0, 2].imshow(Y_test[id][:,:,0])\n",
        "        plt.show()\n",
        "    except TypeError:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnWgnlwMePUp"
      },
      "source": [
        "## Visualize predictions on the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5CnOIQOCRtx"
      },
      "source": [
        "Y_hat_train = model_loaded.predict(X_train, verbose=1)\n",
        "print(Y_hat_train.shape, Y_hat_train.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZG-Ivv1ePUr"
      },
      "source": [
        "for i in range(10):\n",
        "    try:\n",
        "        id = random.randrange(0,len(Y_hat_train))\n",
        "        print(id, X_train[id].shape)\n",
        "        fig, axarr = plt.subplots(nrows=1, ncols=3, squeeze=False, figsize=(20,20))\n",
        "        axarr[0, 0].imshow(X_train[id], aspect='equal')\n",
        "        axarr[0, 1].imshow(Y_hat_train[id][:,:,0], aspect='equal')\n",
        "        axarr[0, 2].imshow(Y_train[id][:,:,0], aspect='equal')\n",
        "        plt.show()\n",
        "    except TypeError:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aykMBfFePUx"
      },
      "source": [
        "## Visualize predictions on the new data!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qI8gMnIvePUz"
      },
      "source": [
        "# Load raster data into numpy array\n",
        "ds = gdal.Open('test/wellington-03m-rural-aerial-photos-2012-2013.tif') #get from https://data.linz.govt.nz/layer/51870-wellington-03m-rural-aerial-photos-2012-2013/   \n",
        "ary = np.dstack([ds.GetRasterBand(i).ReadAsArray() for i in range(1,4)])\n",
        "W_test, _ = ary_to_tiles(ary, shape=(img_height, img_width))\n",
        "W_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7UFh1IRePU8"
      },
      "source": [
        "W_hat_test = model_loaded.predict(W_test, verbose=1)\n",
        "print(W_hat_test.shape, W_hat_test.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6i5KZU5ePU-",
        "scrolled": false
      },
      "source": [
        "for i in range(10):\n",
        "    try:\n",
        "        id = random.randrange(0,len(W_test))\n",
        "        print(id, W_test[id].shape)\n",
        "        fig, axarr = plt.subplots(nrows=1, ncols=3, squeeze=False, figsize=(20,20))\n",
        "        axarr[0, 0].imshow(W_test[id])\n",
        "        axarr[0, 1].imshow(W_hat_test[id][:,:,0])\n",
        "        axarr[0, 2].imshow(W_test[id])\n",
        "        plt.show()\n",
        "    except TypeError:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcjAQ264CRt0"
      },
      "source": [
        "## Live prediction!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZy0FKNECRt0"
      },
      "source": [
        "# Predicting live from your screen! Uses opencv\n",
        "# Go to https://data.linz.govt.nz, Browse data -> Aerial Photos and select one to play with\n",
        "!python ../predict.py 512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQF5GF8FCRt0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqIg7vtdePVE"
      },
      "source": [
        "# Part 5 - Save results\n",
        "Here, we will use the trained keras ConvNet model to create a raster output of the predicted mask.\n",
        "\n",
        "Inputs:\n",
        "    .tif file (RGB image)\n",
        "Outputs:\n",
        "    .tif file (Binary mask)\n",
        "    \n",
        "The output will have floating point pixel values going from 0 to 1 where 0 is 'not a mask' and 1 is 'mask'\n",
        "\n",
        "**Note**: Please make sure you run all the cells in 'Part 0 - Intro' and 'Part 2 - Build model', and the first cell in 'Part 1 - Data input' to load the necessary libraries and functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IA0KZnuQCRt1"
      },
      "source": [
        "# Reload the model\n",
        "img_width, img_height = (480, 480)  #based on common factors for tifs of shape 2880 by 1920, and 4800 by 7200\n",
        "model_loaded = keras_model(img_width=img_width, img_height=img_height)\n",
        "model_loaded.load_weights(filepath=topDir+\"/model/model-weights.hdf5\")\n",
        "model_loaded.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHGmnq-4CRt1"
      },
      "source": [
        "def tiles_to_ary(stacked_ary, final_shape=(2880, 1920)):\n",
        "    \"\"\"\n",
        "    Function to turn a stacked 2D numpy array of shape (tiles, height, width, channels)\n",
        "    into a single 2D numpy array (image) of shape (height, width, channels)\n",
        "    \n",
        "    Outputs a single numpy array suitable for converting into a raster such as a Geotiff\n",
        "    \"\"\"\n",
        "    \n",
        "    assert(len(stacked_ary.shape) == 4)\n",
        "    \n",
        "    ary_height = stacked_ary.shape[1]\n",
        "    ary_width = stacked_ary.shape[2]\n",
        "    \n",
        "    output_ary = np.zeros(shape=final_shape+(stacked_ary.shape[3],))\n",
        "\n",
        "    index = 0\n",
        "    for x_step in range(0, final_shape[1], ary_width):\n",
        "        for y_step in range(0, final_shape[0], ary_height):\n",
        "            x0, x1 = x_step, x_step+ary_width\n",
        "            y0, y1 = y_step, y_step+ary_height\n",
        "            \n",
        "            output_ary[y0:y1,x0:x1] = stacked_ary[index]\n",
        "            index += 1\n",
        "    \n",
        "    return output_ary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "PoJRzx1wCRt2"
      },
      "source": [
        "def tif_to_mask(model, sheet:str, display:bool=False, output:str=None):\n",
        "    ds = gdal.Open('test/{0}.tif'.format(sheet)) #get from https://data.linz.govt.nz/layer/51870-wellington-03m-rural-aerial-photos-2012-2013/\n",
        "    ary = np.dstack([ds.GetRasterBand(i).ReadAsArray() for i in range(1,4)])\n",
        "    print('Input tif has shape:', ary.shape)\n",
        "    \n",
        "    # Convert raster array to tiles, need to ensure it is perfectly tiled!!\n",
        "    W_test, excluded = ary_to_tiles(ary, shape=(img_height, img_width))\n",
        "    if excluded > 0:\n",
        "        print((excluded,), *ary.shape)\n",
        "        raise ValueError('''\n",
        "        Need to ensure perfect tiles to create full mask of input tif! \n",
        "        You have missed {0} tiles from an input raster array of shape ({1},{2}) \n",
        "        Try and find common factors for the input shape {1},{2} \n",
        "        to input into the (img_height, img_width) parameters instead of ({4},{5})\n",
        "        '''.format((excluded,), *ary.shape, img_height, img_width))\n",
        "    \n",
        "    W_hat_test = model_loaded.predict(W_test, verbose=1)\n",
        "    print('Finished predict on {0} tiles of shape ({1},{2}) for: {4}'.format(*W_hat_test.shape + (sheet,)))\n",
        "    \n",
        "    W_hat_ary = tiles_to_ary(stacked_ary=W_hat_test, final_shape=ary.shape[:2])\n",
        "    print(W_hat_ary.shape)\n",
        "    \n",
        "    if display==True:\n",
        "        fig, axarr = plt.subplots(nrows=1, ncols=1, squeeze=False, figsize=(28.8,19.2))\n",
        "        axarr[0, 0].imshow(W_hat_ary[:,:,0])\n",
        "    \n",
        "    if output != None:\n",
        "        print('Output to:', output)\n",
        "        out_ds = gdal_array.SaveArray(W_hat_ary[:,:,0], output, \"gtiff\", prototype=ds)\n",
        "        del out_ds\n",
        "    \n",
        "    del ds  #close opened tif\n",
        "    \n",
        "    return W_hat_ary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "XGmIfW5oCRt2"
      },
      "source": [
        "if not os.path.exists(os.path.join('test', 'predicted')):\n",
        "    os.makedirs(os.path.join('test', 'predicted'))\n",
        "\n",
        "for tif_path in glob.glob(os.path.abspath('test/*.tif')):\n",
        "    sheet = tif_path.split(os.path.sep)[-1][:-4]\n",
        "    tif_to_mask(model=model_loaded, sheet=sheet, output=os.path.dirname(tif_path)+\"{0}predicted{0}{1}_building_outline.tif\".format(os.path.sep, sheet))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlPJh91FCRt3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}