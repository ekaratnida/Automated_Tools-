{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepAdaptiveLearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekaratnida/Automated_Tools-/blob/main/DeepAdaptiveLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2xg3EzWc9H3"
      },
      "source": [
        "# [DEMO] Deep Active Learning in Remote Sensing for data efficient Change Detection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C5e54C0f2If"
      },
      "source": [
        "Code and data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpMuLj-4fN8l"
      },
      "source": [
        "#!git clone https://github.com/previtus/ChangeDetectionProject.git"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MODrgwh5vWx"
      },
      "source": [
        "!pip install mxnet\n",
        "!pip install horovod\n",
        "#!pip install mxboard\n",
        "#!pip install protobuf==3.12.0\n",
        "#!pip uninstall gluoncv\n",
        "!pip install pycm\n",
        "!pip install pathos\n",
        "!pip install rasterio\n",
        "!pip install -U -q PyDrive\n",
        "!pip install tqdm\n",
        "!pip install mock\n",
        "!pip install -U segmentation-models==0.2.0\n",
        "!pip install 'h5py==2.10.0' --force-reinstall\n",
        "!pip install tifffile \n",
        "!pip install imagecodecs "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jMUYcQh3LTy"
      },
      "source": [
        "\"\"\"from colabcode import ColabCode\n",
        "ColabCode()\n",
        "ColabCode(port = 10000, mount_drive=True)\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mpxDmOhnODs"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ptt3UGrTgPHM",
        "outputId": "e3465a98-1d1b-4d7e-8736-6aa30179dde9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IXcgBItNf-Q1",
        "outputId": "853cb306-57a2-444e-e855-c0602b563818"
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR--5b26r7Fy",
        "outputId": "4655762d-1bdb-4bcd-f0cb-ae49fbfa1b4a"
      },
      "source": [
        "%cd drive/MyDrive/ChangeDetection/ChangeDetectionProject"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/ChangeDetection/ChangeDetectionProject\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNf9wQRsfUWW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "84c8fe56-8d56-4017-e027-8366dd2ddeab"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "!pip install tqdm\n",
        "!pip install mock\n",
        "!pip install -U segmentation-models==0.2.0\n",
        "!pip install 'h5py==2.10.0' --force-reinstall\n",
        "!pip install tifffile \n",
        "!pip install imagecodecs "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n",
            "Requirement already satisfied: mock in /usr/local/lib/python3.7/dist-packages (4.0.3)\n",
            "Requirement already up-to-date: segmentation-models==0.2.0 in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: keras>=2.2.0 in /tensorflow-1.15.2/python3.7 (from segmentation-models==0.2.0) (2.3.1)\n",
            "Requirement already satisfied, skipping upgrade: image-classifiers==0.2.0 in /usr/local/lib/python3.7/dist-packages (from segmentation-models==0.2.0) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.7 in /tensorflow-1.15.2/python3.7 (from segmentation-models==0.2.0) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: scikit-image in /usr/local/lib/python3.7/dist-packages (from segmentation-models==0.2.0) (0.16.2)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras>=2.2.0->segmentation-models==0.2.0) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras>=2.2.0->segmentation-models==0.2.0) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras>=2.2.0->segmentation-models==0.2.0) (1.21.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras>=2.2.0->segmentation-models==0.2.0) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.7/dist-packages (from keras>=2.2.0->segmentation-models==0.2.0) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras>=2.2.0->segmentation-models==0.2.0) (1.16.0)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->segmentation-models==0.2.0) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->segmentation-models==0.2.0) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->segmentation-models==0.2.0) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->segmentation-models==0.2.0) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->segmentation-models==0.2.0) (2.5.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->segmentation-models==0.2.0) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->segmentation-models==0.2.0) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->segmentation-models==0.2.0) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->segmentation-models==0.2.0) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image->segmentation-models==0.2.0) (4.4.2)\n",
            "Collecting h5py==2.10.0\n",
            "  Using cached https://files.pythonhosted.org/packages/3f/c0/abde58b837e066bca19a3f7332d9d0493521d7dd6b48248451a9e3fe2214/h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl\n",
            "Collecting numpy>=1.7\n",
            "  Using cached https://files.pythonhosted.org/packages/3f/03/c3526fb4e79a793498829ca570f2f868204ad9a8040afcd72d82a8f121db/numpy-1.21.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n",
            "Collecting six\n",
            "  Using cached https://files.pythonhosted.org/packages/d9/5a/e7c31adbe875f2abbb91bd84cf2dc52d792b5a01506781dbcf25c91daf11/six-1.16.0-py2.py3-none-any.whl\n",
            "\u001b[31mERROR: lucid 0.3.10 requires umap-learn, which is not installed.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.2 has requirement gast==0.2.2, but you'll have gast 0.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: lucid 0.3.10 has requirement numpy<=1.19, but you'll have numpy 1.21.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kapre 0.3.5 has requirement tensorflow>=2.0.0, but you'll have tensorflow 1.15.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.15.0, but you'll have six 1.16.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, six, h5py\n",
            "  Found existing installation: numpy 1.21.0\n",
            "    Uninstalling numpy-1.21.0:\n",
            "      Successfully uninstalled numpy-1.21.0\n",
            "  Found existing installation: six 1.16.0\n",
            "    Uninstalling six-1.16.0:\n",
            "      Successfully uninstalled six-1.16.0\n",
            "  Found existing installation: h5py 2.10.0\n",
            "    Uninstalling h5py-2.10.0:\n",
            "      Successfully uninstalled h5py-2.10.0\n",
            "Successfully installed h5py-2.10.0 numpy-1.21.0 six-1.16.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "six"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tifffile in /usr/local/lib/python3.7/dist-packages (2021.6.14)\n",
            "Requirement already satisfied: numpy>=1.15.1 in /usr/local/lib/python3.7/dist-packages (from tifffile) (1.21.0)\n",
            "Requirement already satisfied: imagecodecs in /usr/local/lib/python3.7/dist-packages (2021.6.8)\n",
            "Requirement already satisfied: numpy>=1.15.1 in /usr/local/lib/python3.7/dist-packages (from imagecodecs) (1.21.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoDY3jLUfgli"
      },
      "source": [
        "# Trained models and sample data:\n",
        "\n",
        "#BACKBONE = 'resnet34'\n",
        "#custom_weights_file = \"model_UNet-Vgg16_DSM_in01_95percOfTrain_8batch_100ep_dsm01proper.h5\" # None\n",
        "#custom_weights_file = \"imagenet\"\n",
        "\n",
        "downloaded = drive.CreateFile({'id': '1Yv8ik5xdbkB8b4etMW60_uP3Kw2fdA57'})\n",
        "downloaded.GetContentFile('weightsModel2_cleanManual_100ep_ImagenetWgenetW_resnet50-16batch_Augmentation1to1_ClassWeights1to3_TestVal_[KFold_0z5].h5')\n",
        "#downloaded.GetContentFile(custom_weights_file)\n",
        "\n",
        "downloaded = drive.CreateFile({'id': '1V-V73WLOfzYtXEb95LDIlkTz84437hEC'})\n",
        "downloaded.GetContentFile('samples.zip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyKHdKOV_9sm"
      },
      "source": [
        "Load and prepare the model and datapreprocessor (with values from train set for this exact model)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFdYfPHygbFT"
      },
      "source": [
        "import numpy \n",
        "import tifffile as tiff \n",
        "import matplotlib, os\n",
        "\n",
        "if not('DISPLAY' in os.environ):\n",
        "    matplotlib.use(\"Agg\")\n",
        "\n",
        "import DataLoader, DataPreprocesser, Dataset, Debugger, Settings, ModelHandler, Evaluator\n",
        "from timeit import default_timer as timer\n",
        "from datetime import *\n",
        "\n",
        "months = [\"unk\",\"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"]\n",
        "month = (months[datetime.now().month])\n",
        "day = str(datetime.now().day)\n",
        "\n",
        "import Settings\n",
        "import mock\n",
        "args = mock.Mock()\n",
        "args.name = \"Run-\"+month+\"-\"+day\n",
        "args.KFOLDS = \"5\"\n",
        "args.FOLD_I = \"0\"\n",
        "args.model_backend = \"resnet50\"\n",
        "args.train_epochs = \"100\"\n",
        "args.train_batch = \"16\"\n",
        "\n",
        "if True:\n",
        "\n",
        "    print(args)\n",
        "\n",
        "    settings = Settings.Settings(args)\n",
        "    settings.TestDataset_Fold_Index = int(args.FOLD_I)\n",
        "    settings.TestDataset_K_Folds = int(args.KFOLDS)\n",
        "    \n",
        "    assert settings.TestDataset_Fold_Index < settings.TestDataset_K_Folds\n",
        "    \n",
        "    kfold_txt = \"KFold_\"+str(settings.TestDataset_Fold_Index)+\"z\"+str(settings.TestDataset_K_Folds)\n",
        "    print(kfold_txt)\n",
        "\n",
        "    # resnet 101 approx 5-6 hours (per fold - might be a bit less ...)\n",
        "    # resnet 50  approx 3-4 hours\n",
        "    model_txt = \"cleanManual_\"+args.train_epochs+\"ep_ImagenetWgenetW_\"+args.model_backend+\"-\"+args.train_batch+\"batch_Augmentation1to1_ClassWeights1to3_TestVal\"\n",
        "    print(model_txt)\n",
        "\n",
        "    datapreprocessor = DataPreprocesser.DataPreprocesser(settings, 3)\n",
        "    \n",
        "    datapreprocessor.zeroweighting_L_means_per_channel = [128.97206, 110.9056, 118.42062, 95.341675]\n",
        "    datapreprocessor.zeroweighting_L_stds_per_channel = [52.3949, 50.5274, 45.264183, 39.304928]\n",
        "\n",
        "    datapreprocessor.zeroweighting_R_means_per_channel = [153.98038, 119.60732, 118.80906, 112.634346]\n",
        "    datapreprocessor.zeroweighting_R_stds_per_channel = [54.66875, 48.597534, 46.159058, 44.616997]\n",
        "    \n",
        "    dataset = Dataset.Dataset(settings, init_source = -1)\n",
        "    evaluator = Evaluator.Evaluator(settings)\n",
        "\n",
        "    show = False\n",
        "    save = True\n",
        "\n",
        "    settings.model_backend = args.model_backend\n",
        "    settings.train_epochs = int(args.train_epochs)\n",
        "    settings.train_batch = int(args.train_batch)\n",
        "    model = ModelHandler.ModelHandler(settings, dataset)\n",
        "\n",
        "    model.model.load(\"weightsModel2_cleanManual_100ep_ImagenetWgenetW_resnet50-16batch_Augmentation1to1_ClassWeights1to3_TestVal_[KFold_0z5].h5\")\n",
        "    #model.model.load(\"\")\n",
        "    model.model.model.summary()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-db0YmuM4k8k"
      },
      "source": [
        "#from Model2_builder import *\n",
        "#BACKBONE = 'resnet34'\n",
        "#custom_weights_file = \"model_UNet-Resnet34_DSM_in01_95percOfTrain_8batch_100ep_dsm01proper.h5\" # None\n",
        "#custom_weights_file = \"imagenet\"\n",
        "#model = SiameseUnet() #BACKBONE, encoder_weights=custom_weights_file, classes=3, activation='softmax', input_shape=(256, 256, 3))\n",
        "#print(\"Model loaded:\")\n",
        "#print(\"model.input\", model.input)\n",
        "#print(\"model.output\", model.output)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfysNZn_6nxQ"
      },
      "source": [
        "!unzip -q samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-n2hyINAU4D"
      },
      "source": [
        "!ls samples/*.PNG"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8COJOtdKAXcq"
      },
      "source": [
        "Predict on an example of data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWFHOr1tlUaJ"
      },
      "source": [
        "from skimage import io\n",
        "import numpy as np\n",
        "import PIL\n",
        "import IPython\n",
        "import tifffile as tiff\n",
        "\n",
        "def load_raster_image(filename):\n",
        "  if 'tif' in filename:\n",
        "    img = io.imread(filename, plugin='pil')\n",
        "  else:\n",
        "    img = io.imread(filename)\n",
        "  matrix = img #[0:512,0:512,:] \n",
        "  #print(matrix)\n",
        "  arr = np.asarray(matrix)\n",
        "  return arr\n",
        "\n",
        "def predict(left_path, right_path):\n",
        "    i1 = load_raster_image(left_path)\n",
        "    i2 = load_raster_image(right_path)\n",
        "\n",
        "    print(\"before \", i1.shape)\n",
        "    print(\"after \", i2.shape)\n",
        "\n",
        "    test_L = np.asarray([i1]); print(test_L.shape)\n",
        "    test_R = np.asarray([i2])\n",
        "    orig_L, orig_R = test_L, test_R\n",
        "    data = [test_L, test_R]\n",
        "\n",
        "    # 2 PREPROCESSED IT\n",
        "    data = datapreprocessor.apply_on_a_set_nondestructively(data, no_labels = True)\n",
        "    test_L, test_R = data\n",
        "\n",
        "    if test_L.shape[3] > 3:\n",
        "      # 3 channels only - rgb\n",
        "      test_L = test_L[:,:,:,1:4]\n",
        "      test_R = test_R[:,:,:,1:4]\n",
        "\n",
        "    models = [model.model.model]\n",
        "    # 3 PREDICTION\n",
        "    predicted = models[0].predict(x=[test_L, test_R], batch_size=4)\n",
        "\n",
        "    #predicted = model.predict(x=[test_L, test_R], batch_size=4)\n",
        "\n",
        "\n",
        "    predicted = predicted[:, :, :, 1]\n",
        "    p = predicted[0]\n",
        "    l = orig_L[0,:,:,1:4]\n",
        "    r = orig_R[0,:,:,1:4]\n",
        "\n",
        "    return p, l, r\n",
        "\n",
        "#path1 = \"samples/strip1-2012_6467.PNG\"  \n",
        "#path2 = \"samples/strip1-2015_6467.PNG\"\n",
        "\n",
        "# 1 LOADED NEW DATA FOR PREDICTION\n",
        "#path1 = \"samples/before256.png\" #strip1-2012_6467.PNG\"\n",
        "#path2 = \"samples/after256.png\" #strip1-2015_6467.PNG\"\n",
        "\n",
        "path1 = \"samples/BeforeMegaBKK2.tif\"\n",
        "path2 = \"samples/AfterMegaBKK2.tif\"\n",
        "\n",
        "p, l, r = predict(path1, path2)\n",
        "\n",
        "new_p = PIL.Image.fromarray(p*255.0)\n",
        "if new_p.mode != 'RGB':\n",
        "  new_p = new_p.convert('RGB')\n",
        "new_p.save('samples/OutputMegaBKK2.tif')\n",
        "IPython.display.display(new_p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TpL-AVoAxDt"
      },
      "source": [
        "And to double check what were the original inputs?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOhgvWGVdY3A"
      },
      "source": [
        "def save_image(image, name):\n",
        "    PIL.Image.fromarray(image, 'RGB').save(name)\n",
        "\n",
        "def show_three(l,r,p):\n",
        "    # Show the triple:\n",
        "    new_p = PIL.Image.fromarray(p*255.0)\n",
        "    if new_p.mode != 'RGB':\n",
        "        new_p = new_p.convert('RGB')\n",
        "    rgbpred = np.asarray(new_p)\n",
        "\n",
        "    #print(rgbpred.shape)\n",
        "    #print(l.shape)\n",
        "    #print(r.shape)\n",
        "\n",
        "    horizontal = np.concatenate(([l,r,rgbpred]), axis = 1)\n",
        "    save_image(horizontal, \"demo.png\")\n",
        "\n",
        "    from IPython.display import Image\n",
        "    return Image(\"demo.png\", width=512)\n",
        "\n",
        "show_three(l,r,p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zN-CC2VhM-h"
      },
      "source": [
        "!ls samples/*.PNG"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Wc0l6WihVIt"
      },
      "source": [
        "# Demo predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_mOhvMjgh4L"
      },
      "source": [
        "# samples/strip1-2012_6467.PNG  samples/strip1-2015_6467.PNG\n",
        "p1 = \"samples/strip1-2012_6472.PNG\"#\"samples/before256.png\"\n",
        "p2 = \"samples/strip1-2015_6472.PNG\"#\"samples/after256.png\"\n",
        "\n",
        "p,l,r = predict(p1, p2)\n",
        "show_three(l,r,p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH_Psr_nNdm8"
      },
      "source": [
        "#   \n",
        "p1 = \"samples/strip1-2012_6467.PNG\" #samples/Before3.tiff\"#\"samples/before256.png\"\n",
        "p2 = \"samples/strip1-2015_6467.PNG\" #samples/After3.tiff\"#\"samples/after256.png\"\n",
        "\n",
        "p,l,r = predict(p1, p2)\n",
        "show_three(l,r,p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfcVD0AXiCfk"
      },
      "source": [
        "# samples/strip1-2012_6472.PNG  samples/strip1-2015_6472.PNG\n",
        "\n",
        "p1 = \"samples/strip1-2012_6518.PNG\"\n",
        "p2 = \"samples/strip1-2015_6518.PNG\"\n",
        "\n",
        "p,l,r = predict(p1, p2)\n",
        "show_three(l,r,p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEqvEqAtiZPh"
      },
      "source": [
        "# samples/strip1-2012_6889.PNG  samples/strip1-2015_6889.PNG\n",
        "\n",
        "p1 = \"samples/strip1-2012_6889.PNG\"\n",
        "p2 = \"samples/strip1-2015_6889.PNG\"\n",
        "\n",
        "p,l,r = predict(p1, p2)\n",
        "show_three(l,r,p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns-c1qIfiQDt"
      },
      "source": [
        "# samples/strip1-2012_6518.PNG  samples/strip1-2015_6518.PNG\n",
        "\n",
        "p1 = \"samples/strip1-2012_6518.PNG\"\n",
        "p2 = \"samples/strip1-2015_6518.PNG\"\n",
        "\n",
        "p,l,r = predict(p1, p2)\n",
        "show_three(l,r,p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7o7E-H8idSL"
      },
      "source": [
        "### Bonus note:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkQSRwXfiUCt"
      },
      "source": [
        "# If we give the model a wrong ordering of the pair of samples (from the year 2012 and year 2015)\n",
        "# ... we will still get a prediction, however the accuracy suffers (see the above prediction for comparison)\n",
        "# This is due to visual quality differences between the two sets (such as different weather and time of capture of the data).\n",
        "\n",
        "p,l,r = predict(p2, p1)\n",
        "show_three(l,r,p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmPsBvo5glFc",
        "outputId": "9fc08791-5536-41a0-d134-11c2241c96cf"
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJwRbb5r4OWZ",
        "outputId": "5e799240-5c05-4a30-c3e6-7a09eaec788e"
      },
      "source": [
        "!python main.py"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.7/dist-packages/classification_models/resnext/__init__.py:4: UserWarning: Current ResNext models are deprecated, use keras.applications ResNeXt models\n",
            "  warnings.warn('Current ResNext models are deprecated, '\n",
            "Namespace(FOLD_I='0', KFOLDS='5', model_backend='resnet50', name='Run-jun-24', train_batch='8', train_epochs='100')\n",
            "KFold_0z5\n",
            "cleanManual_100ep_ImagenetWgenetW_resnet50-8batch_Augmentation1to1_ClassWeights1to3_TestVal\n",
            "Init manually from data and labels\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "2021-06-24 05:33:05.163669: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2021-06-24 05:33:05.174597: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-06-24 05:33:05.174652: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (f2ccfbf4f061): /proc/driver/nvidia/version does not exist\n",
            "2021-06-24 05:33:05.175058: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\n",
            "2021-06-24 05:33:05.205408: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000129999 Hz\n",
            "2021-06-24 05:33:05.205733: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560269542a00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2021-06-24 05:33:05.205769: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Downloading data from https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet50_imagenet_1000_no_top.h5\n",
            "94593024/94592056 [==============================] - 2s 0us/step\n",
            "Model loaded:\n",
            "model.input [<tf.Tensor 'input_1:0' shape=(?, ?, ?, 3) dtype=float32>, <tf.Tensor 'input_2:0' shape=(?, ?, ?, 3) dtype=float32>]\n",
            "model.output Tensor(\"softmax/truediv:0\", shape=(?, ?, ?, 2), dtype=float32)\n",
            "WARNING:tensorflow:From /content/drive/MyDrive/ChangeDetection/ChangeDetectionProject/loss_weighted_crossentropy.py:24: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Tensor(\"loss/softmax_loss/loss/mul:0\", shape=(?, ?, ?, 2), dtype=float32)\n",
            "Tensor(\"loss/softmax_loss/loss/mul_2:0\", shape=(?, ?, ?, 2), dtype=float32)\n",
            "Model: \"u-resnet50\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, None, None, 3 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, None, None, 3 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model_2 (Model)                 [(None, None, None,  23546057    input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatHighLvlFeat (Concatenate) (None, None, None, 4 0           model_2[1][0]                    \n",
            "                                                                 model_2[2][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage0_upsample (UpSamp (None, None, None, 4 0           concatHighLvlFeat[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, None, None, 6 0           decoder_stage0_upsample[0][0]    \n",
            "                                                                 model_2[1][1]                    \n",
            "                                                                 model_2[2][1]                    \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage0_conv1 (Conv2D)   (None, None, None, 2 14155776    concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage0_bn1 (BatchNormal (None, None, None, 2 1024        decoder_stage0_conv1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage0_relu1 (Activatio (None, None, None, 2 0           decoder_stage0_bn1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage0_conv2 (Conv2D)   (None, None, None, 2 589824      decoder_stage0_relu1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage0_bn2 (BatchNormal (None, None, None, 2 1024        decoder_stage0_conv2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage0_relu2 (Activatio (None, None, None, 2 0           decoder_stage0_bn2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage1_upsample (UpSamp (None, None, None, 2 0           decoder_stage0_relu2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, None, None, 1 0           decoder_stage1_upsample[0][0]    \n",
            "                                                                 model_2[1][2]                    \n",
            "                                                                 model_2[2][2]                    \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage1_conv1 (Conv2D)   (None, None, None, 1 1474560     concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage1_bn1 (BatchNormal (None, None, None, 1 512         decoder_stage1_conv1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage1_relu1 (Activatio (None, None, None, 1 0           decoder_stage1_bn1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage1_conv2 (Conv2D)   (None, None, None, 1 147456      decoder_stage1_relu1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage1_bn2 (BatchNormal (None, None, None, 1 512         decoder_stage1_conv2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage1_relu2 (Activatio (None, None, None, 1 0           decoder_stage1_bn2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage2_upsample (UpSamp (None, None, None, 1 0           decoder_stage1_relu2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, None, None, 6 0           decoder_stage2_upsample[0][0]    \n",
            "                                                                 model_2[1][3]                    \n",
            "                                                                 model_2[2][3]                    \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage2_conv1 (Conv2D)   (None, None, None, 6 368640      concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage2_bn1 (BatchNormal (None, None, None, 6 256         decoder_stage2_conv1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage2_relu1 (Activatio (None, None, None, 6 0           decoder_stage2_bn1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage2_conv2 (Conv2D)   (None, None, None, 6 36864       decoder_stage2_relu1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage2_bn2 (BatchNormal (None, None, None, 6 256         decoder_stage2_conv2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage2_relu2 (Activatio (None, None, None, 6 0           decoder_stage2_bn2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage3_upsample (UpSamp (None, None, None, 6 0           decoder_stage2_relu2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, None, None, 1 0           decoder_stage3_upsample[0][0]    \n",
            "                                                                 model_2[1][4]                    \n",
            "                                                                 model_2[2][4]                    \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage3_conv1 (Conv2D)   (None, None, None, 3 55296       concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage3_bn1 (BatchNormal (None, None, None, 3 128         decoder_stage3_conv1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage3_relu1 (Activatio (None, None, None, 3 0           decoder_stage3_bn1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage3_conv2 (Conv2D)   (None, None, None, 3 9216        decoder_stage3_relu1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage3_bn2 (BatchNormal (None, None, None, 3 128         decoder_stage3_conv2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage3_relu2 (Activatio (None, None, None, 3 0           decoder_stage3_bn2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage4_upsample (UpSamp (None, None, None, 3 0           decoder_stage3_relu2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage4_conv1 (Conv2D)   (None, None, None, 1 4608        decoder_stage4_upsample[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage4_bn1 (BatchNormal (None, None, None, 1 64          decoder_stage4_conv1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage4_relu1 (Activatio (None, None, None, 1 0           decoder_stage4_bn1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage4_conv2 (Conv2D)   (None, None, None, 1 2304        decoder_stage4_relu1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage4_bn2 (BatchNormal (None, None, None, 1 64          decoder_stage4_conv2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "decoder_stage4_relu2 (Activatio (None, None, None, 1 0           decoder_stage4_bn2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "final_conv (Conv2D)             (None, None, None, 2 290         decoder_stage4_relu2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "softmax (Activation)            (None, None, None, 2 0           final_conv[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 40,394,859\n",
            "Trainable params: 40,347,301\n",
            "Non-trainable params: 47,558\n",
            "__________________________________________________________________________________________________\n",
            "Train\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 129, in <module>\n",
            "    main(args)\n",
            "  File \"main.py\", line 54, in main\n",
            "    model.model.train(show=show,save=save)\n",
            "  File \"/content/drive/MyDrive/ChangeDetection/ChangeDetectionProject/Model2_SiamUnet_Encoder.py\", line 91, in train\n",
            "    train_L, train_R, train_V = self.dataset.train\n",
            "AttributeError: 'Dataset' object has no attribute 'train'\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}